<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Dantong Niu</title>

    <meta name="author" content="Dantong Niu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/tutu.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Dantong Niu
                </p>
                <p>I am a second-year Ph.D. student advised by Prof. <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a> at UC Berkeley.
                </p>
                <p>
                  I develop vision-language models for robotics.
                </p>
                <p style="text-align:center">
                  <a href="mailto:niudantong.88@gmail.com">Email</a> &nbsp;/&nbsp;
<!--                  <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp;-->
<!--                  <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp;-->
                  <a href="https://scholar.google.com/citations?user=AzlUrvUAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
<!--                  <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp;-->
<!--                  <a href="https://bsky.app/profile/jonbarron.bsky.social">Bluesky</a> &nbsp;/&nbsp;-->
                  <a href="https://github.com/Dantong88/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:35%;max-width:35%">
                 <a href="images/dantong.jpeg"><img style="width:95%;max-width:95%" alt="profile photo" src="images/dantong.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Selected Publications</h2>
                <p>
                  I am fortunate that my PhD research unfolds in an era where the explosion of groundbreaking LLMs and VLMs is revolutionizing robotics learning with unprecedented possibilities.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>

            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/roboprompt.png" alt="fast-texture" width="200" height="140">
              </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2410.12782">
                  <papertitle>In-Context Learning Enables Robot Action Prediction in LLMs</papertitle>
                </a>
            <br>
            <a href="https://davidyyd.github.io/">Yida Yin*</a>,
            <a href="https://zekaiwang04.github.io">Zekai Wang*</a>,
            <a href="https://scholar.google.com/citations?hl=en&user=1_IIcds8es4C">Yuvan Sharma</a>,
            <strong>Dantong Niu</strong>,
            <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
            <a href="https://roeiherz.github.io/">Roei Herzig</a>
            <br>
            <em>ICRA</em> 2025
            <br>
              <a href="https://github.com/davidyyd/roboprompt">project page</a> /
                <a href="https://github.com/davidyyd/roboprompt">code</a> /
                <a href="https://arxiv.org/abs/2410.12782">paper</a>

          </td>
          </tr>



        <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/llarva.gif" alt="fast-texture" width="200" height="140">
              </td>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2406.11815">
                  <papertitle>LLARVA: Vision-Action Instruction Tuning Enhances Robot Learning</papertitle>
                  <img src="images/llarva.png" alt="fast-texture" width=20" height="20">
                </a>
                <br>
                  <strong>Dantong Niu*</strong>,
                  Yuvan Sharma*, Giscard Biamby, Jerome Quenum, Yutong Bai, Baifeng Shi,
                <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>&dagger;,
                  <a href="https://roeiherz.github.io/">Roei Herzig</a>&dagger;
                <br>
                  <em> Conference on Robot Learning (CoRL), 2024 </em>
<!--                <em> Conference on Empirical Methods in Natural Language Processing (EMNLP) </em> , 2023-->
                 <br>
                <a href="https://llarva24.github.io/">project page</a> /
                <a href="https://github.com/Dantong88/LLARVA">code</a> /
                <a href="https://arxiv.org/abs/2406.11815">paper</a>
                <br>
                <p></p>
                <p>
                    We propose LLARVA, a model trained with a novel instruction tuning method that
                    leverages structured prompts to unify a range of robotic configurations
                    and introduces the concept of visual traces to further align the vision and action spaces.

                </p>
              </td>
        </tr>


        <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/U2seg.jpg" alt="fast-texture" width="200" height="140">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2312.17243">
                  <papertitle>U2Seg: Unsupervised Universal Image Segmentation</papertitle>
<!--                  <img src="images/new.gif" alt="fast-texture" width="25" height="11">-->
                </a>
                <br>
                  <strong>Dantong Niu*</strong>,
                  <a href="https://people.eecs.berkeley.edu/~xdwang/">Xudong Wang*</a>,
                  Xinyang Han*,
                  Long Lian,
                  Roei Herzig,
                <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>
                <br>
                  <em> IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) </em> , 2024
<!--                <em> Conference on Empirical Methods in Natural Language Processing (EMNLP) </em> , 2023-->
                 <br>
<!--                <a href="https://github.com/u2seg/U2Seg?tab=readme-ov-file">project page</a> /-->
                <a href="https://u2seg.github.io/">project page</a> /
                <a href="https://github.com/u2seg/U2Seg">code</a> /
                <a href="https://arxiv.org/abs/2312.17243">paper</a>
                <br>
                <p></p>
                <p>
                    We present U2Seg, a unified framework for Unsupervised Universal image Segmentation
                    that consistently outperforms previous state-of-the-art methods.
                </p>
              </td>

        </tr>





          </tbody></table>

          
<!--        <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;">-->
<!--        <tbody>-->
<!--            <tr>-->
<!--              <td>-->
<!--                <h2>Miscellanea</h2>-->
<!--              </td>-->
<!--            </tr>-->
<!--       </tbody>-->
<!--        </table>-->
<!--          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--            -->
<!--           -->
<!--            <tr>-->
<!--              <td align="center" style="padding:16px;width:20%;vertical-align:middle">-->
<!--						     <div class="colored-box" style="background-color: #fcb97d;">-->
<!--								 <h2>Micropapers</h2>-->
<!--								 </div>-->
<!--              </td>-->
<!--              <td style="padding:8px;width:80%;vertical-align:middle">-->
<!--                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>-->
<!--                <br>-->
<!--                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>-->
<!--                <br>-->
<!--                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>-->
<!--                <br>-->
<!--                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>-->
<!--              </td>-->
<!--            </tr>-->


<!--            <tr>-->
<!--              <td align="center" style="padding:16px;width:20%;vertical-align:middle">-->
<!--						     <div class="colored-box" style="background-color: #aaba9e;">-->
<!--								 <h2>Recorded Talks</h2>-->
<!--								 </div>-->
<!--              </td>-->
<!--              <td style="padding:8px;width:80%;vertical-align:center">-->
<!--                <a href="https://www.youtube.com/watch?v=h9vq_65eDas">View Dependent Podcast, 2024</a>-->
<!--								<br>-->
<!--                <a href="https://www.youtube.com/watch?v=4tDhYsFuEqo">Bay Area Robotics Symposium, 2023-->
<!--</a>-->
<!--								<br>-->
<!--                <a href="https://youtu.be/TvWkwDYtBP4?t=7604">EGSR Keynote, 2021</a>-->
<!--								<br>-->
<!--								<a href="https://www.youtube.com/watch?v=nRyOzHpcr4Q">TUM AI Lecture Series, 2020</a>-->
<!--								<br>-->
<!--								<a href="https://www.youtube.com/watch?v=HfJpQCBTqZs">Vision & Graphics Seminar at MIT, 2020</a>-->
<!--              </td>-->
<!--            </tr>-->

<!--            <tr>-->
<!--              <td align="center" style="padding:16px;width:20%;vertical-align:middle">-->
<!--						     <div class="colored-box" style="background-color: #c6b89e;">-->
<!--								 <h2>Academic Service</h2>-->
<!--								 </div>-->
<!--              </td>-->
<!--              <td style="padding:8px;width:80%;vertical-align:center">-->
<!--                <a href="https://iccv.thecvf.com/">Lead Area Chair, ICCV 2025</a>-->
<!--                <br>-->
<!--                <a href="https://cvpr.thecvf.com/Conferences/2025/Organizers">Lead Area Chair, CVPR 2025</a>-->
<!--                <br>-->
<!--                <a href="https://cvpr.thecvf.com/Conferences/2024/Organizers">Area Chair, CVPR 2024</a>-->
<!--                <br>-->
<!--                <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>-->
<!--                <br>-->
<!--                <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>-->
<!--                <br>-->
<!--                <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Award Committee Member, CVPR 2021</a>-->
<!--                <br>-->
<!--                <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>-->
<!--                <br>-->
<!--                <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>-->
<!--              </td>-->
<!--            </tr>-->
<!--						-->
<!--						-->
<!--           -->
<!--            <tr>-->
<!--              <td align="center" style="padding:16px;width:20%;vertical-align:middle">-->
<!--						     <div class="colored-box" style="background-color: #edd892;">-->
<!--								 <h2>Teaching</h2>-->
<!--								 </div>-->
<!--              </td>-->
<!--              <td style="padding:8px;width:80%;vertical-align:center">-->
<!--                <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>-->
<!--                <br>-->
<!--                <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>-->
<!--                <br>-->
<!--                <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>-->
<!--              </td>-->
<!--            </tr>-->
<!--            -->
<!--          </tbody></table>-->
<!--          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--            <tr>-->
<!--              <td style="padding:0px">-->
<!--                <br>-->
<!--                <p style="text-align:right;font-size:small;">-->
<!--                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.-->
<!--                </p>-->
<!--              </td>-->
<!--            </tr>-->
<!--          </tbody></table>-->
<!--        </td>-->
<!--      </tr>-->
<!--    </table>-->
<!--  </body>-->
<!--</html>-->
