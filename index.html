<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Dantong Niu</title>

    <meta name="author" content="Dantong Niu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/tutu.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Dantong Niu
                </p>
                <p>I am a second-year Ph.D. student advised by Prof. <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a> at UC Berkeley.
                </p>
                <p>
                  I develop vision-language models for robotics.
                </p>
                <p style="text-align:center">
                  <a href="mailto:niudantong.88@gmail.com">Email</a> &nbsp;/&nbsp;
<!--                  <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp;-->
<!--                  <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp;-->
                  <a href="https://scholar.google.com/citations?user=AzlUrvUAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
<!--                  <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp;-->
<!--                  <a href="https://bsky.app/profile/jonbarron.bsky.social">Bluesky</a> &nbsp;/&nbsp;-->
                  <a href="https://github.com/Dantong88/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:35%;max-width:35%">
                 <a href="images/dantong.jpeg"><img style="width:95%;max-width:95%" alt="profile photo" src="images/dantong.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Selected Publications</h2>
                <p>
                  I am fortunate that my PhD research unfolds in an era where the explosion of groundbreaking LLMs and VLMs is revolutionizing robotics learning with unprecedented possibilities.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>

            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/arm4r.png" alt="fast-texture" width="200" height="140">
              </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2502.13142">
                  <papertitle>Pre-training Auto-regressive Robotic Models with 4D Representations</papertitle>
                </a>
            <br>
                <strong>Dantong Niu*</strong>,
                Yuvan Sharma*,
                <a href="https://haoruxue.github.io/">Haoru Xue</a>,
                <a href="https://scholar.google.com/citations?user=s0Fof5IAAAAJ&hl=en"</a>Giscard Biamby,
                <a href="https://www.junyi42.com/">Junyi Zhang</a>,
                Ziteng Ji,
                <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>&dagger;,
                  <a href="https://roeiherz.github.io/">Roei Herzig</a>&dagger;
                <br>
                  <em> Forty-Second International Conference on Machine Learning (ICML), 2025 </em>
<!--                <em> Conference on Empirical Methods in Natural Language Processing (EMNLP) </em> , 2023-->
                 <br>
<!--                <a href="https://llarva24.github.io/">project page</a> /-->
<!--                <a href="https://github.com/Dantong88/LLARVA">code</a> /-->
                <a href="https://arxiv.org/abs/2502.13142">arxiv</a>
          </td>
          </tr>

          <tr>

            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/roboprompt.png" alt="fast-texture" width="200" height="140">
              </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2410.12782">
                  <papertitle>In-Context Learning Enables Robot Action Prediction in LLMs</papertitle>
                </a>
            <br>
            <a href="https://davidyyd.github.io/">Yida Yin*</a>,
            <a href="https://zekaiwang04.github.io">Zekai Wang*</a>,
            <a href="https://scholar.google.com/citations?hl=en&user=1_IIcds8es4C">Yuvan Sharma</a>,
            <strong>Dantong Niu</strong>,
            <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
            <a href="https://roeiherz.github.io/">Roei Herzig</a>
            <br>
            <em>IEEE International Conf. on Robotics and Automation (ICRA), 2025 </em>
            <br>
              <a href="https://github.com/davidyyd/roboprompt">project page</a> /
                <a href="https://github.com/davidyyd/roboprompt">code</a> /
                <a href="https://arxiv.org/abs/2410.12782">paper</a>

          </td>
          </tr>



        <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/llarva.gif" alt="fast-texture" width="200" height="140">
              </td>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2406.11815">
                  <papertitle>LLARVA: Vision-Action Instruction Tuning Enhances Robot Learning</papertitle>
                  <img src="images/llarva.png" alt="fast-texture" width=20" height="20">
                </a>
                <br>
                  <strong>Dantong Niu*</strong>,
                  Yuvan Sharma*, Giscard Biamby, Jerome Quenum, Yutong Bai, Baifeng Shi,
                <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>&dagger;,
                  <a href="https://roeiherz.github.io/">Roei Herzig</a>&dagger;
                <br>
                  <em> Conference on Robot Learning (CoRL), 2024 </em>
<!--                <em> Conference on Empirical Methods in Natural Language Processing (EMNLP) </em> , 2023-->
                 <br>
                <a href="https://llarva24.github.io/">project page</a> /
                <a href="https://github.com/Dantong88/LLARVA">code</a> /
                <a href="https://arxiv.org/abs/2406.11815">paper</a>
                <br>
                <p></p>
                <p>
                    We propose LLARVA, a model trained with a novel instruction tuning method that
                    leverages structured prompts to unify a range of robotic configurations
                    and introduces the concept of visual traces to further align the vision and action spaces.

                </p>
              </td>
        </tr>


        <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/u2seg.jpg" alt="fast-texture" width="200" height="140">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2312.17243">
                  <papertitle>U2Seg: Unsupervised Universal Image Segmentation</papertitle>
<!--                  <img src="images/new.gif" alt="fast-texture" width="25" height="11">-->
                </a>
                <br>
                  <strong>Dantong Niu*</strong>,
                  <a href="https://people.eecs.berkeley.edu/~xdwang/">Xudong Wang*</a>,
                  Xinyang Han*,
                  <a href="https://tonylian.com/">Long Lian</a>,
                  Roei Herzig,
                <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>
                <br>
                  <em> IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2024 </em>
<!--                <em> Conference on Empirical Methods in Natural Language Processing (EMNLP) </em> , 2023-->
                 <br>
<!--                <a href="https://github.com/u2seg/U2Seg?tab=readme-ov-file">project page</a> /-->
                <a href="https://u2seg.github.io/">project page</a> /
                <a href="https://github.com/u2seg/U2Seg">code</a> /
                <a href="https://arxiv.org/abs/2312.17243">paper</a>
                <br>
                <p></p>
                <p>
                    We present U2Seg, a unified framework for Unsupervised Universal image Segmentation
                    that consistently outperforms previous state-of-the-art methods.
                </p>
              </td>

        </tr>





          </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Previous Work</h2>
                <p>
                  Before starting my Ph.D. studies, I worked on general machine learning and vision topics such as object segmentation,  adversarial attacks, and video understanding.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/MA.png" alt="fast-texture" width="200" height="140">
              </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <a href="https://proceedings.neurips.cc/paper/2021/hash/db9eeb7e678863649bce209842e0d164-Abstract.html">
                  <papertitle>Morié Attack (MA): A New Potential Risk of Screen Photos</papertitle>
                </a>
            <br>
            <strong>Dantong Niu*</strong>,
            <a href="https://ruohaoguo.github.io/">Ruohao Guo*</a>,
            <a href="https://yisenwang.github.io/">Yisen Wang</a>
            <br>
            <em>NeurIPS, 2021</em>
            <br>
                <a href="https://github.com/Dantong88/Moire_Attack">code</a> /
                <a href="https://proceedings.neurips.cc/paper/2021/hash/db9eeb7e678863649bce209842e0d164-Abstract.html">paper</a>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/SOTR.png" alt="fast-texture" width="200" height="140">
              </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2108.06747">
                  <papertitle>SOTR: Segmenting Objects with Transformers</papertitle>
                </a>
            <br>
            <a href="https://ruohaoguo.github.io/">Ruohao Guo*</a>,
            <strong>Dantong Niu*</strong>,
            <a href="https://github.com/QuLiao1117">Liao Qu</a>,
            <a href="https://faculty.cau.edu.cn/lzb_en/">Zhenbo Li</a>
            <br>
            <em> IEEE Conf. on International Conference on Computer Vision (ICCV), 2021</em>
            <br>
                <a href="https://github.com/easton-cau/SOTR">code</a> /
                <a href="https://arxiv.org/abs/2108.06747">paper</a>

            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/advdrop.png" alt="fast-texture" width="200" height="140">
              </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2108.09034">
                  <papertitle>AdvDrop: Adversarial Attack to DNNs by Dropping Information</papertitle>
                </a>
            <br>

            Ranjie Duan,
            <a href="https://ieeexplore.ieee.org/author/37086866065">Yuefeng Chen</a>,
            <strong>Dantong Niu*</strong>,
            Yun Yang,
            A. K. Qin,
            Yuan He
            <br>
            <em> IEEE Conf. on International Conference on Computer Vision (ICCV), 2021 </em>
            <br>
                <a href="https://github.com/RjDuan/AdvDrop">code</a> /
                <a href="https://ieeexplore.ieee.org/author/37086866065">paper</a>

            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/leafmask.png" alt="fast-texture" width="200" height="140">
              </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2108.03568">
                  <papertitle>LeafMask: Towards Greater Accuracy on Leaf Segmentation</papertitle>
                </a>
            <br>
            <a href="https://ruohaoguo.github.io/">Ruohao Guo*</a>,
            <a href="https://github.com/QuLiao1117">Liao Qu</a>,
            <strong>Dantong Niu*</strong>,
            <a href="https://faculty.cau.edu.cn/lzb_en/">Zhenbo Li</a>,
            Jun Yue
            <br>
            <em> IEEE Conf. on International Conference on Computer Vision Workshops (ICCVW), 2021 </em>
            <br>
                <a href="https://github.com/easton-cau/LeafMask">code</a> /
                <a href="https://arxiv.org/abs/2108.06747">paper</a>

            </td>
          </tr>





          </tbody></table>

    </table>